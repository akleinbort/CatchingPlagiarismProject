{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1\">Problem Statement</a></span></li><li><span><a href=\"#Executive-Summary\" data-toc-modified-id=\"Executive-Summary-2\">Executive Summary</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\">Imports</a></span></li><li><span><a href=\"#Logging\" data-toc-modified-id=\"Logging-4\">Logging</a></span></li><li><span><a href=\"#Web-scraping\" data-toc-modified-id=\"Web-scraping-5\">Web scraping</a></span><ul class=\"toc-item\"><li><span><a href=\"#Automated-scraping-functions\" data-toc-modified-id=\"Automated-scraping-functions-5.1\">Automated scraping functions</a></span></li><li><span><a href=\"#Today-I-Learned-Scrape\" data-toc-modified-id=\"Today-I-Learned-Scrape-5.2\">Today I Learned Scrape</a></span></li><li><span><a href=\"#Shower-Thoughts-Scrape\" data-toc-modified-id=\"Shower-Thoughts-Scrape-5.3\">Shower Thoughts Scrape</a></span></li></ul></li><li><span><a href=\"#Concat-&amp;-Save-Joint-DataFrames\" data-toc-modified-id=\"Concat-&amp;-Save-Joint-DataFrames-6\">Concat &amp; Save Joint DataFrames</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "A plagiarism company has come to us hoping we can identify plagiarized concepts rather than just copy-pasted words. Using NLP, we can look for clues in language structure and content to differentiate between original and learned ideas.\n",
    "\n",
    "![pikachudetective](../Images/pikachu.jpg)\n",
    "\n",
    "In order to tackle this problem we will scrape reddit for learned and original content using the subreddits Today I Learned and Shower Thoughts respectively. We then use predictive modeling to classify the content as learned or original, and evaluate the results so we can deploy an appropriate separation threshold.\n",
    "\n",
    "Simple Unit Testing and Logging are included for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "In order to build an idea-plagiarism model, we used **Reddit data** from **ShowerThoughts** and **Today I Learned**, which contains **1504 observations**. This information pertains to subreddit titles pulled on October 17, 2019. \n",
    "\n",
    "\n",
    "We used several models to classify this data into their categories, the best one being a **MLPClassifier**. In order to optimize this model, we used an extensive grid search with FeatureUnion vectorizers (tfidf and cvec).\n",
    "\n",
    "\n",
    "The resulting train and tests score were 0.84 and **0.86**. At the 0.5 threshold, the **precision was 0.83** and the **recall was 0.92**.\n",
    "\n",
    "We then set up a simple threshold modifier to suit the clients needs. For instance, we might want to be more severe with papers for publication than we would want to be on high school students. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import nltk\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pytest\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from nltk.stem                       import WordNetLemmatizer\n",
    "from nltk.tokenize                   import RegexpTokenizer\n",
    "from bs4                             import BeautifulSoup  \n",
    "from nltk.corpus                     import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline                import Pipeline,FeatureUnion\n",
    "from sklearn.model_selection         import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble                import VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_selection       import VarianceThreshold\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes             import MultinomialNB, GaussianNB\n",
    "from sklearn.neural_network          import MLPClassifier\n",
    "from sklearn.preprocessing           import StandardScaler\n",
    "from sklearn.svm                     import SVC\n",
    "from sklearn.metrics                 import confusion_matrix,classification_report\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we would want to deploy the scrape to be used on live data, we want to logg our actions to supervise our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A log file is created to keep track of everything\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.ERROR, stream=sys.stdout)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "#To avoid long web scraping outputs\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nice format for the logger\n",
    "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated scraping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#web requests\n",
    "def request(url_base, user):\n",
    "    #User\n",
    "    user_agent = {\"User-agent\": user}      # header to prevent 429 error\n",
    "    #Requests\n",
    "    res = requests.get(url = url_base,\n",
    "                   headers = user_agent)\n",
    "    #check status - should be 2xx\n",
    "    print(res.status_code)\n",
    "    return res, res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling\n",
    "def pulling(url_base, user_agent):\n",
    "    # empty list\n",
    "    posts = []\n",
    "    \n",
    "    # Instantiate after string\n",
    "    after = None\n",
    "    \n",
    "    for pull_num in range(30):\n",
    "        \n",
    "        # Count message\n",
    "        print(f'Pull number {pull_num+1}')\n",
    "        \n",
    "        # url updates\n",
    "        if after == None:  \n",
    "            new_url = url_base\n",
    "        else:\n",
    "            new_url = url_base+'?after='+after\n",
    "        \n",
    "        # request\n",
    "        res = requests.get(url = new_url,\n",
    "                           headers = user_agent)\n",
    "    \n",
    "        # data extraction\n",
    "        if res.status_code == 200:\n",
    "            json_data = res.json()                     \n",
    "            posts.extend(json_data['data']['children']) \n",
    "            \n",
    "        # update string\n",
    "            after = json_data['data']['after']\n",
    "        else:\n",
    "            print(\"We've run into an error. The status code is:\", res.status_code)\n",
    "            break\n",
    "    \n",
    "        # wait\n",
    "        time.sleep(2)\n",
    "        print(\"We have:\", len(set([p['data']['name'] for p in posts])), \"posts in this subreddit\")\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame\n",
    "def scrape_df(posts, file_name, save_path):\n",
    "    df=pd.DataFrame(columns=[])\n",
    "    redits=[]\n",
    "    titles=[]\n",
    "    texts=[]\n",
    "    for val,post in enumerate(posts):\n",
    "        redits.append(posts[val]['data']['subreddit'])\n",
    "        titles.append(posts[val]['data']['title'])\n",
    "        texts.append(posts[val]['data']['selftext'])\n",
    "    df['subred']=redits\n",
    "    df['title']=titles\n",
    "    df['text']=texts\n",
    "    #df_name=df\n",
    "    df.to_csv(f'{save_path}/{file_name}');\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today I Learned Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "res, json = request(url_base=\"https://www.reddit.com/r/todayilearned.json\", user='ambar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pull number 1\n",
      "We have: 25 posts in this subreddit\n",
      "Pull number 2\n",
      "We have: 50 posts in this subreddit\n",
      "Pull number 3\n",
      "We have: 75 posts in this subreddit\n",
      "Pull number 4\n",
      "We have: 100 posts in this subreddit\n",
      "Pull number 5\n",
      "We have: 125 posts in this subreddit\n",
      "Pull number 6\n",
      "We have: 150 posts in this subreddit\n",
      "Pull number 7\n",
      "We have: 175 posts in this subreddit\n",
      "Pull number 8\n",
      "We have: 200 posts in this subreddit\n",
      "Pull number 9\n",
      "We have: 225 posts in this subreddit\n",
      "Pull number 10\n",
      "We have: 250 posts in this subreddit\n",
      "Pull number 11\n",
      "We have: 275 posts in this subreddit\n",
      "Pull number 12\n",
      "We have: 300 posts in this subreddit\n",
      "Pull number 13\n",
      "We have: 325 posts in this subreddit\n",
      "Pull number 14\n",
      "We have: 350 posts in this subreddit\n",
      "Pull number 15\n",
      "We have: 375 posts in this subreddit\n",
      "Pull number 16\n",
      "We have: 400 posts in this subreddit\n",
      "Pull number 17\n",
      "We have: 425 posts in this subreddit\n",
      "Pull number 18\n",
      "We have: 450 posts in this subreddit\n",
      "Pull number 19\n",
      "We have: 475 posts in this subreddit\n",
      "Pull number 20\n",
      "We have: 500 posts in this subreddit\n",
      "Pull number 21\n",
      "We have: 525 posts in this subreddit\n",
      "Pull number 22\n",
      "We have: 550 posts in this subreddit\n",
      "Pull number 23\n",
      "We have: 575 posts in this subreddit\n",
      "Pull number 24\n",
      "We have: 600 posts in this subreddit\n",
      "Pull number 25\n",
      "We have: 625 posts in this subreddit\n",
      "Pull number 26\n",
      "We have: 638 posts in this subreddit\n",
      "Pull number 27\n",
      "We have: 638 posts in this subreddit\n",
      "Pull number 28\n",
      "We have: 638 posts in this subreddit\n",
      "Pull number 29\n",
      "We have: 638 posts in this subreddit\n",
      "Pull number 30\n",
      "We have: 638 posts in this subreddit\n"
     ]
    }
   ],
   "source": [
    "til_posts = pulling(url_base=\"https://www.reddit.com/r/todayilearned.json\", user_agent={\"User-agent\": 'ambar'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "til_df=scrape_df(til_posts, 'til_posts.csv', '../Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shower Thoughts Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "res, json = request(url_base=\"https://www.reddit.com/r/Showerthoughts.json\", user='ambar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pull number 1\n",
      "We have: 27 posts in this subreddit\n",
      "Pull number 2\n",
      "We have: 52 posts in this subreddit\n",
      "Pull number 3\n",
      "We have: 77 posts in this subreddit\n",
      "Pull number 4\n",
      "We have: 102 posts in this subreddit\n",
      "Pull number 5\n",
      "We have: 127 posts in this subreddit\n",
      "Pull number 6\n",
      "We have: 152 posts in this subreddit\n",
      "Pull number 7\n",
      "We have: 177 posts in this subreddit\n",
      "Pull number 8\n",
      "We have: 202 posts in this subreddit\n",
      "Pull number 9\n",
      "We have: 227 posts in this subreddit\n",
      "Pull number 10\n",
      "We have: 252 posts in this subreddit\n",
      "Pull number 11\n",
      "We have: 277 posts in this subreddit\n",
      "Pull number 12\n",
      "We have: 302 posts in this subreddit\n",
      "Pull number 13\n",
      "We have: 327 posts in this subreddit\n",
      "Pull number 14\n",
      "We have: 352 posts in this subreddit\n",
      "Pull number 15\n",
      "We have: 377 posts in this subreddit\n",
      "Pull number 16\n",
      "We have: 402 posts in this subreddit\n",
      "Pull number 17\n",
      "We have: 427 posts in this subreddit\n",
      "Pull number 18\n",
      "We have: 452 posts in this subreddit\n",
      "Pull number 19\n",
      "We have: 477 posts in this subreddit\n",
      "Pull number 20\n",
      "We have: 502 posts in this subreddit\n",
      "Pull number 21\n",
      "We have: 527 posts in this subreddit\n",
      "Pull number 22\n",
      "We have: 552 posts in this subreddit\n",
      "Pull number 23\n",
      "We have: 577 posts in this subreddit\n",
      "Pull number 24\n",
      "We have: 602 posts in this subreddit\n",
      "Pull number 25\n",
      "We have: 627 posts in this subreddit\n",
      "Pull number 26\n",
      "We have: 651 posts in this subreddit\n",
      "Pull number 27\n",
      "We have: 651 posts in this subreddit\n",
      "Pull number 28\n",
      "We have: 651 posts in this subreddit\n",
      "Pull number 29\n",
      "We have: 651 posts in this subreddit\n",
      "Pull number 30\n",
      "We have: 651 posts in this subreddit\n"
     ]
    }
   ],
   "source": [
    "st_posts = pulling(url_base=\"https://www.reddit.com/r/Showerthoughts.json\", user_agent={\"User-agent\": 'ambar'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_df=scrape_df(st_posts, 'st_posts.csv', '../Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat & Save Joint DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "til_df=til_df.append(st_df)\n",
    "FullRedditScrape=til_df.reset_index().drop(columns='index')\n",
    "FullRedditScrape.to_csv('../Data/FullRedditScrape.csv');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
